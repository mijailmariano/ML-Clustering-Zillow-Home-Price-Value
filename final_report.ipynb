{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zillow Clustering Analysis\n",
    "**Artifact: Jupyter Notebook Report**\n",
    "\n",
    "Created by: Mijail Q. Mariano\n",
    "\n",
    "Presented: Tuesday, August 09th 2022\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Libraries & Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mlp\n",
    "# mlp.rcParams['figure.dpi'] = 300\n",
    "\n",
    "# diasbling warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# importing key libraries\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# numpy import\n",
    "import numpy as np\n",
    "\n",
    "# scipy import\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "# datetime module for home transaction dates\n",
    "import datetime\n",
    "\n",
    "# importing acquire module\n",
    "import acquire\n",
    "from acquire import get_zillow_dataset, \\\n",
    "                    clean_zillow_dataset, \\\n",
    "                    age_of_homes, \\\n",
    "                    get_lower_and_upper_bounds, \\\n",
    "                    zillow_outliers, \\\n",
    "                    clean_months, \\\n",
    "                    null_df, \\\n",
    "                    train_iterative_imputer, \\\n",
    "                    impute_val_and_test, \\\n",
    "                    train_validate_test_split, \\\n",
    "                    get_transaction_quarters, \\\n",
    "                    get_dummy_dataframes, \\\n",
    "                    get_cluster_dummy\n",
    "\n",
    "# importing data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "# sklearn data science library\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import SelectKBest, RFE\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### **``Project Scope and Objectives:``**\n",
    "\n",
    "<u>``Scope:``</u> Apply machine learning clustering techniques to better segment Zillow's real-estate data and reduce the overall error in predicted home evaluations as measured by Zillow's \"Zestimate\". \n",
    "\n",
    "<u>``Objectives:``</u>\n",
    "\n",
    "1. Acquire and clean the Zillow dataset\n",
    "2. Apply and treat outliers in the dataset (make recommendations)\n",
    "3. Identify and treat missing values in the dataset\n",
    "4. Generate statistical hypotheses for testing\n",
    "5. Use dataset features to create clusters\n",
    "6. Generate visualizations to interpret clusters and make selections for modeling\n",
    "7. Use clusters to model logerror predictions and evaluate results\n",
    "8. Make final recommendations\n",
    "\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **``Data Acquisition and Preparation``**\n",
    "\n",
    "**``Key Highlights``**\n",
    "\n",
    "- Used domain knowledge and research to focus on key questions for analysis\n",
    "- Renamed and converted columns/features to proper data type\n",
    "- Dropped initial records and features with > 80% Null values\n",
    "- Added columns:\n",
    "    - \"Home Age\"\n",
    "    - \"Transactions by Month\"\n",
    "    - \"Transactions by Quarter\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*these homes represented homes larger than the majority homes in the dataset, therefore making the analysis or future prediction less accurate or obscured when including these records in the analysis.* *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquiring and preparing initial zillow dataset\n",
    "\n",
    "df = get_zillow_dataset()\n",
    "df = clean_zillow_dataset(df)\n",
    "df = age_of_homes(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "```Dataset Summary Statistics:```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe info:\n",
    "\n",
    "sorted_cols = df.columns.sort_values()\n",
    "df[sorted_cols].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe summary statistics:\n",
    "\n",
    "summary_stats = df.describe().T\n",
    "summary_stats[\"range\"] = summary_stats[\"max\"] - summary_stats[\"min\"]\n",
    "summary_stats.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "```Handling Outliers:```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using \"iqr\" method to determine lower and upper bounds for continuous variables\n",
    "\n",
    "get_lower_and_upper_bounds(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning dataset for outliers at lower and upper bounds\n",
    "\n",
    "df = zillow_outliers(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>**Note on Outliers:**</u>\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding transactions by month column\n",
    "\n",
    "df = clean_months(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### **``Splitting the Zillow Dataset for Hypothesis Testing:``**\n",
    "\n",
    "Highlights:\n",
    "\n",
    "- initial dataset: (52238, 14)\n",
    "- dataset after cleaning: (38293, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the Zillow dataset\n",
    "\n",
    "train, validate, test = train_validate_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detecting and handling missing values in the training dataset\n",
    "\n",
    "null_df = null_df(train)\n",
    "null_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling remaining null values using sklearn's iterative imputer\n",
    "\n",
    "train_imputed = train_iterative_imputer(train)\n",
    "train_imputed.isnull().sum() # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling validate and test datasets\n",
    "\n",
    "validate_imputed, test_imputed = impute_val_and_test(train, validate, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### <u>``Analyzing the Target Variable (logerror)``</u>\n",
    "\n",
    "*logerror = log(Predicted: Zestimate) âˆ’ log(Actual: Home Transaction Price)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting alpha for hypothesis tests\n",
    "\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining logerror by county and home value\n",
    "\n",
    "plt.figure(figsize = (20, 3))\n",
    "sns.set(font_scale = .5)\n",
    "\n",
    "p = sns.jointplot(\n",
    "    data = train_imputed.sample(5000, random_state = 14),\n",
    "    x = \"home_value\", \n",
    "    y = \"logerror\", \n",
    "    hue = \"county_by_fips\",\n",
    "    s = 4)\n",
    "\n",
    "p.fig.suptitle(\"Logerror by County Home Value\")\n",
    "p.fig.tight_layout()\n",
    "p.fig.subplots_adjust(top = .95) # Reduce plot to make room \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining logerror distribution by county (exclusive of outliers)\n",
    "\n",
    "county_lst = list(train_imputed[\"county_by_fips\"].unique())\n",
    "\n",
    "for ele in county_lst:\n",
    "    print(f'County: {ele}')\n",
    "    print(f'logerror: {round(train_imputed[train_imputed[\"county_by_fips\"] == ele].logerror.var(), 5)}')\n",
    "    print('-------------------------')\n",
    "\n",
    "    plt.figure(figsize = (8,3))\n",
    "    plt.xlim(-.2, .2)\n",
    "\n",
    "    plt.title('Distribution of logerror for {}'.format(ele))\n",
    "    plt.hist(train_imputed[train_imputed[\"county_by_fips\"] == ele].logerror, bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**```Hypothesis Question Number 1: Is there a difference in logerror across transaction month?```**\n",
    "\n",
    "Given the many economic factors that may influence the housing market, I belive that this could also lead to challenges in accurate and timely home evaluations - thus leading to over, or under estimating a home's true value.\n",
    "\n",
    "**Null Hypothesis:** \"There's **not** a statistical logerror difference across transaction months.\"\n",
    "\n",
    "**Alternative Hypothesis:** \"There **is** a statistical logerror difference across transaction months.\"\n",
    "\n",
    "$\\alpha$: 0.05\n",
    "\n",
    "- Monthly logerror variances are relatively equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting monthly logerror\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.set(font_scale = 0.6)\n",
    "sns.barplot(x = \"transaction_month\",\n",
    "    y = \"logerror\", \n",
    "    data = train_imputed,\n",
    "    order = [\n",
    "    'January',\n",
    "    'February',\n",
    "    'March',\n",
    "    'April',\n",
    "    'May',\n",
    "    'June',\n",
    "    'July',\n",
    "    'August',\n",
    "    'September'],\n",
    "    palette = \"crest_r\",\n",
    "    ci = 95)\n",
    "\n",
    "\n",
    "plt.title(\"Yearly Glance: Log Error by Month\")\n",
    "plt.xlabel(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating transaction yearly quarter columns (future testing)\n",
    "\n",
    "# train dataset\n",
    "train_imputed[\"q1_transaction\"] = (train_imputed[\"transaction_month\"] == \"January\") | (train_imputed[\"transaction_month\"] == \"February\") | (train_imputed[\"transaction_month\"] == \"March\")\n",
    "train_imputed[\"q2_transaction\"] = (train_imputed[\"transaction_month\"] == \"April\") | (train_imputed[\"transaction_month\"] == \"May\") | (train_imputed[\"transaction_month\"] == \"June\")\n",
    "train_imputed[\"q3_transaction\"] = (train_imputed[\"transaction_month\"] == \"July\") | (train_imputed[\"transaction_month\"] == \"August\") | (train_imputed[\"transaction_month\"] == \"September\")\n",
    "\n",
    "# validate dataset\n",
    "validate_imputed[\"q1_transaction\"] = (validate_imputed[\"transaction_month\"] == \"January\") | (validate_imputed[\"transaction_month\"] == \"February\") | (validate_imputed[\"transaction_month\"] == \"March\")\n",
    "validate_imputed[\"q2_transaction\"] = (validate_imputed[\"transaction_month\"] == \"April\") | (validate_imputed[\"transaction_month\"] == \"May\") | (validate_imputed[\"transaction_month\"] == \"June\")\n",
    "validate_imputed[\"q3_transaction\"] = (validate_imputed[\"transaction_month\"] == \"July\") | (validate_imputed[\"transaction_month\"] == \"August\") | (validate_imputed[\"transaction_month\"] == \"September\")\n",
    "\n",
    "# test dataset\n",
    "test_imputed[\"q1_transaction\"] = (test_imputed[\"transaction_month\"] == \"January\") | (test_imputed[\"transaction_month\"] == \"February\") | (test_imputed[\"transaction_month\"] == \"March\")\n",
    "test_imputed[\"q2_transaction\"] = (test_imputed[\"transaction_month\"] == \"April\") | (test_imputed[\"transaction_month\"] == \"May\") | (test_imputed[\"transaction_month\"] == \"June\")\n",
    "test_imputed[\"q3_transaction\"] = (test_imputed[\"transaction_month\"] == \"July\") | (test_imputed[\"transaction_month\"] == \"August\") | (test_imputed[\"transaction_month\"] == \"September\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melting quarter transaction columns\n",
    "\n",
    "train_imputed[\"transaction_quarter\"] = train_imputed[[\"q1_transaction\", \"q2_transaction\", \"q3_transaction\"]].idxmax(1).to_frame('transaction_quarter')\n",
    "validate_imputed[\"transaction_quarter\"] = validate_imputed[[\"q1_transaction\", \"q2_transaction\", \"q3_transaction\"]].idxmax(1).to_frame('transaction_quarter')\n",
    "test_imputed[\"transaction_quarter\"] = test_imputed[[\"q1_transaction\", \"q2_transaction\", \"q3_transaction\"]].idxmax(1).to_frame('transaction_quarter')\n",
    "\n",
    "# dropping redundant columns\n",
    "\n",
    "train_imputed.drop(columns = [\"q1_transaction\", \"q2_transaction\", \"q3_transaction\"], inplace = True)\n",
    "validate_imputed.drop(columns = [\"q1_transaction\", \"q2_transaction\", \"q3_transaction\"], inplace = True)\n",
    "test_imputed.drop(columns = [\"q1_transaction\", \"q2_transaction\", \"q3_transaction\"], inplace = True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA hypothesis test for: transactions by quarter\n",
    "\n",
    "f_statistic, p_value = stats.f_oneway(\n",
    "    train_imputed[train_imputed[\"transaction_month\"] == \"January\"].logerror,\n",
    "    train_imputed[train_imputed[\"transaction_month\"] == \"February\"].logerror,\n",
    "    train_imputed[train_imputed[\"transaction_month\"] == \"March\"].logerror,\n",
    "    train_imputed[train_imputed[\"transaction_month\"] == \"April\"].logerror,\n",
    "    train_imputed[train_imputed[\"transaction_month\"] == \"May\"].logerror,\n",
    "    train_imputed[train_imputed[\"transaction_month\"] == \"June\"].logerror,\n",
    "    train_imputed[train_imputed[\"transaction_month\"] == \"July\"].logerror,\n",
    "    train_imputed[train_imputed[\"transaction_month\"] == \"August\"].logerror,\n",
    "    train_imputed[train_imputed[\"transaction_month\"] == \"September\"].logerror)\n",
    "\n",
    "# comparing the counties p_value to alpha\n",
    "print(f'f statistic = {f_statistic.round(5)}')\n",
    "print(f'p value = {p_value.round(5)}')\n",
    "print(\"-----------------\")\n",
    "\n",
    "if p_value < alpha:\n",
    "    print('We reject the null hypothesis. Means are different across the groups.')\n",
    "else:\n",
    "    print('We fail to reject the null hypothesis. Means are similar across the groups.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "```Hypothesis Question Number 2: Is there a difference in logerror across home sizes (binned living sq-feet)?```\n",
    "\n",
    "Social and cultural changes such as the age/period of first-time families, interests in more sustainable and smaller eco-friendly lifestyles can all play a role in determining the size value of a home. I believe factors such as these can undoubtedbly make predicting the precise true value of a home more difficult.\n",
    "\n",
    "**Null Hypothesis:** There's **not** a statistical logerror difference across home sizes.\"\n",
    "\n",
    "**Althernative Hypothesis:** There **is** a statistical logerror difference across home sizes.\"\n",
    "\n",
    "$\\alpha$: 0.05\n",
    "\n",
    "- Home Size logerror variances are not equal.\n",
    "\n",
    "<u>**Binned Home Sizes:**</u>\n",
    "\n",
    "* 360 - 1241 sq. ft (smallest)\n",
    "* 1241 - 1566 sq. ft\n",
    "* 1566 - 2037 sq. ft\n",
    "* 2037 - 3855 sq ft (largest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating new home size column\n",
    "\n",
    "sq_ft_labels = [\"360_to_1240_sqfeet\", \"1241_to_1565_sqfeet\", \"1566_to_2036_sqfeet\", \"2037_to_3855_sqfeet\"]\n",
    "\n",
    "train_imputed[\"living_sqfeet_binned\"] = pd.qcut(\n",
    "    train_imputed[\"living_sq_feet\"], \\\n",
    "    q = 4, \\\n",
    "    labels = sq_ft_labels)\n",
    "\n",
    "validate_imputed[\"living_sqfeet_binned\"] = pd.qcut(\n",
    "    validate_imputed[\"living_sq_feet\"], \\\n",
    "    q = 4, \\\n",
    "    labels = sq_ft_labels)\n",
    "\n",
    "test_imputed[\"living_sqfeet_binned\"] = pd.qcut(\n",
    "    test_imputed[\"living_sq_feet\"], \\\n",
    "    q = 4, \\\n",
    "    labels = sq_ft_labels)\n",
    "\n",
    "train_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting logerror by home size\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.set(font_scale = 0.6)\n",
    "sns.barplot(x = \"living_sqfeet_binned\",\n",
    "    y = \"logerror\", \n",
    "    data = train_imputed,\n",
    "    order = [\n",
    "        \"2037_to_3855_sqfeet\", \n",
    "        \"1566_to_2036_sqfeet\", \n",
    "        \"1241_to_1565_sqfeet\",\n",
    "        \"360_to_1240_sqfeet\"],\n",
    "    palette = \"crest_r\",\n",
    "    ci = 95)\n",
    "\n",
    "plt.title(\"Log Error by Home Size\")\n",
    "plt.xlabel(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA hypothesis test for: home sizes\n",
    "\n",
    "f_statistic, p_value = stats.f_oneway(\n",
    "    train_imputed[train_imputed[\"living_sqfeet_binned\"] == \"360_to_1240_sqfeet\"].logerror,\n",
    "    train_imputed[train_imputed[\"living_sqfeet_binned\"] == \"1241_to_1565_sqfeet\"].logerror,\n",
    "    train_imputed[train_imputed[\"living_sqfeet_binned\"] == \"1566_to_2036_sqfeet\"].logerror,\n",
    "    train_imputed[train_imputed[\"living_sqfeet_binned\"] == \"2037_to_3855_sqfeet\"].logerror)\n",
    "\n",
    "# comparing the counties p_value to alpha\n",
    "print(f'f statistic = {f_statistic.round(5)}')\n",
    "print(f'p value = {p_value.round(5)}')\n",
    "print(\"-----------------\")\n",
    "\n",
    "if p_value < alpha:\n",
    "    print('We reject the null hypothesis. Means are different across the groups.')\n",
    "else:\n",
    "    print('We fail to reject the null hypothesis. Means are similar across the groups.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "```Hypothesis Question Number 3: Is there a difference in logerror across building era?```\n",
    "\n",
    "As time passes so do the architectural design methods and the kinds of homes that are built. For example, colonial style homes may be reminiscent of a 17th-18th century time period. \n",
    "\n",
    "A mid-century modern design home - may provide a feeling of both nostalgia and future creativity. In either case, I believe that as time passes so do the building styles of home and ultimately which home styles are more prevailing in current times. Unfortunately, similar to home styles - these preference trends are just as difficult to predict but could be valuable to understand.\n",
    "\n",
    "**Null Hypothesis:** There's **not** a statistical logerror difference across building era.\"\n",
    "\n",
    "**Althernative Hypothesis:** There **is** a statistical logerror difference across building era.\"\n",
    "\n",
    "$\\alpha$: 0.05\n",
    "\n",
    "- Build Era logerror variances are not equal.\n",
    "\n",
    "<u>**Home Building Eras:**</u>\n",
    "\n",
    "* 1977 - 2015: New Century\n",
    "* 1960 - 1976: Late 20th Century\n",
    "* 1950 - 1959: Mid 20th Century\n",
    "* 1907 - 1949: Early 20th Century"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new home build era column \n",
    "\n",
    "age_labels = [\"new_century\", \"late_20th_century\", \"mid_20th_century\", \"early_20th_century\"]\n",
    "\n",
    "train_imputed[\"home_age_binned\"] = pd.qcut(\n",
    "    train_imputed[\"home_age\"],\n",
    "    q = 4,\n",
    "    labels = age_labels)\n",
    "\n",
    "validate_imputed[\"home_age_binned\"] = pd.qcut(\n",
    "    validate_imputed[\"home_age\"], \n",
    "    q = 4, \n",
    "    labels = age_labels)\n",
    "\n",
    "test_imputed[\"home_age_binned\"] = pd.qcut(\n",
    "    test_imputed[\"home_age\"], \n",
    "    q = 4, \n",
    "    labels = age_labels)\n",
    "\n",
    "train_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting logerror by building era w/20% Confidence Interval\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.set(font_scale = 0.6)\n",
    "sns.barplot(\n",
    "    x = \"home_age_binned\",\n",
    "    y = \"logerror\",\n",
    "    data = train_imputed,\n",
    "    order = [\"new_century\", \"late_20th_century\", \"mid_20th_century\", \"early_20th_century\"], \n",
    "    ci = 20,\n",
    "    palette = \"magma_r\")\n",
    "\n",
    "plt.title(\"Differences in Home Era Preferences\")\n",
    "plt.xlabel(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA hypothesis test for: home era\n",
    "\n",
    "f_statistic, p_value = stats.f_oneway(\n",
    "    train_imputed[train_imputed[\"home_age_binned\"] == \"new_century\"].logerror,\n",
    "    train_imputed[train_imputed[\"home_age_binned\"] == \"late_20th_century\"].logerror,\n",
    "    train_imputed[train_imputed[\"home_age_binned\"] == \"mid_20th_century\"].logerror,\n",
    "    train_imputed[train_imputed[\"home_age_binned\"] == \"early_20th_century\"].logerror)\n",
    "    \n",
    "\n",
    "# comparing the counties p_value to alpha\n",
    "print(f'f statistic = {f_statistic.round(5)}')\n",
    "print(f'p value = {p_value.round(5)}')\n",
    "print(\"-----------------\")\n",
    "\n",
    "if p_value < alpha:\n",
    "    print('We reject the null hypothesis. Means are different across the groups.')\n",
    "else:\n",
    "    print('We fail to reject the null hypothesis. Means are similar across the groups.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### <u>``Scaling Data and Clustering Features``</u>\n",
    "\n",
    "**Highlights:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummy variables for clustering\n",
    "\n",
    "train_dummy, validate_dummy, test_dummy = get_dummy_dataframes(train_imputed, validate_imputed, test_imputed)\n",
    "train_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling plot using sklearn's MinMaxScaler\n",
    "\n",
    "cont_lst = train_dummy.select_dtypes(exclude = [\"object\", \"category\", \"uint8\", \"int64\", \"bool\"]).columns.tolist()\n",
    "cont_lst = [ele for ele in cont_lst if ele not in (\"logerror\", \"home_value\")]\n",
    "\n",
    "for col in cont_lst:\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_dummy[[col]])\n",
    "\n",
    "    x_scaled = scaler.transform(train_dummy[[col]])\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(121)\n",
    "    sns.histplot(train_dummy[[col]], bins = 25, edgecolor = 'black', label = col)\n",
    "    \n",
    "    plt.title(f'Original: {col}')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    ax = sns.histplot(x_scaled, bins=25, edgecolor = 'black', label = \"scaled\")\n",
    "    \n",
    "    # removing axes scientific notation \n",
    "    ax.ticklabel_format(style = \"plain\") \n",
    "    plt.title(f'Scaled: {col}')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling necessary features in datasets\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_dummy[cont_lst])\n",
    "\n",
    "# dummy datasets\n",
    "train_dummy[cont_lst] = scaler.transform(train_dummy[cont_lst])\n",
    "validate_dummy[cont_lst] = scaler.transform(validate_dummy[cont_lst])\n",
    "test_dummy[cont_lst] = scaler.transform(test_dummy[cont_lst])\n",
    "\n",
    "# non-dummy datasets\n",
    "train_imputed[cont_lst] = scaler.transform(train_imputed[cont_lst])\n",
    "validate_imputed[cont_lst] = scaler.transform(validate_imputed[cont_lst])\n",
    "test_imputed[cont_lst] = scaler.transform(test_imputed[cont_lst])\n",
    "\n",
    "train_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### **``Cluster Number 1: Monthly Cluster``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting features to cluster on \n",
    "\n",
    "X_month = train_dummy[[\n",
    "    'property_sq_feet',\n",
    "    'transaction_month_January',\n",
    "    'transaction_month_February',\n",
    "    'transaction_month_March',\n",
    "    'transaction_month_April',\n",
    "    'transaction_month_May',\n",
    "    'transaction_month_June',\n",
    "    'transaction_month_July',\n",
    "    'transaction_month_August',\n",
    "    'transaction_month_September'\n",
    "    ]]\n",
    "\n",
    "# validate df\n",
    "X_val = validate_dummy[[\n",
    "    'property_sq_feet',\n",
    "    'transaction_month_January',\n",
    "    'transaction_month_February',\n",
    "    'transaction_month_March',\n",
    "    'transaction_month_April',\n",
    "    'transaction_month_May',\n",
    "    'transaction_month_June',\n",
    "    'transaction_month_July',\n",
    "    'transaction_month_August',\n",
    "    'transaction_month_September'\n",
    "    ]]\n",
    "\n",
    "# test df\n",
    "X_test = test_dummy[[\n",
    "    'property_sq_feet',\n",
    "    'transaction_month_January',\n",
    "    'transaction_month_February',\n",
    "    'transaction_month_March',\n",
    "    'transaction_month_April',\n",
    "    'transaction_month_May',\n",
    "    'transaction_month_June',\n",
    "    'transaction_month_July',\n",
    "    'transaction_month_August',\n",
    "    'transaction_month_September'\n",
    "    ]]\n",
    "\n",
    "X_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial fitting of KMeans cluster\n",
    "\n",
    "kmeans = KMeans(n_clusters = 9)\n",
    "kmeans.fit(X_month)\n",
    "\n",
    "train_clusters = kmeans.predict(X_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans Elbow-method\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    pd.Series({k: KMeans(k).fit(X_month).inertia_ for k in range(1, 12)}).plot(marker = 'o')\n",
    "    plt.xticks(range(2, 13))\n",
    "    \n",
    "    plt.ylim(0, 21000)\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('inertia')\n",
    "    plt.title('Change in inertia as k increases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting inertia for number of KMeans clusters\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(13, 13), sharex=True, sharey=True)\n",
    "\n",
    "for ax, k in zip(axs.ravel(), range(4, 13)):\n",
    "    X1 = X_month.sample(1000, random_state = 14)\n",
    "\n",
    "    # creating and fitting KMeans\n",
    "    clusters = KMeans(k).fit(X1).predict(X1)\n",
    "\n",
    "    X1[\"logerror\"] = train_dummy[\"logerror\"]\n",
    "\n",
    "    ax.scatter(\n",
    "        X1[\"property_sq_feet\"],\n",
    "        X1[\"logerror\"],\n",
    "        c = clusters,\n",
    "        s = 2)\n",
    "\n",
    "    ax.set(title='k = {}'.format(k), xlabel = 'property_sq_feet', ylabel = 'logerror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating cluster for ea. dataset \n",
    "\n",
    "train_month_clusters = kmeans.predict(X_month)\n",
    "val_month_clusters = kmeans.predict(X_val)\n",
    "test_month_clusters = kmeans.predict(X_test)\n",
    "\n",
    "pd.DataFrame(train_month_clusters).rename(columns = {0: \"Cluster Sample\"}).sample(10, random_state = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding clusters back to original datasets\n",
    "\n",
    "train_imputed[\"month_clusters\"] = train_month_clusters\n",
    "validate_imputed[\"month_clusters\"] = val_month_clusters\n",
    "test_imputed[\"month_clusters\"] = test_month_clusters\n",
    "\n",
    "train_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing how month clusters compare against actual month and home age\n",
    "\n",
    "sns.set(font_scale = 0.6)\n",
    "plt.subplots(1, 2, figsize = (16, 6), sharex = True, sharey = False)\n",
    "\n",
    "plt.subplot(121)\n",
    "sns.scatterplot(\n",
    "        x = \"property_sq_feet\",\n",
    "        y = \"logerror\",\n",
    "        data = train_imputed.sample(1000, random_state = 14), \n",
    "        hue = \"transaction_month\",\n",
    "        # hue_order = [\n",
    "        # 'January',\n",
    "        # 'August',\n",
    "        # 'April',\n",
    "        # 'June',\n",
    "        # 'May',\n",
    "        # 'July',\n",
    "        # 'March',\n",
    "        # 'February',\n",
    "        # 'September'],\n",
    "        palette = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6']\n",
    ")\n",
    "\n",
    "plt.title(\"Actual: Transaction Month & Property Sq. Feet\")\n",
    "plt.legend(loc = 'upper left', ncol = 3)\n",
    "plt.ylim(-.2, .2)\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.scatterplot(\n",
    "        x = \"property_sq_feet\",\n",
    "        y = \"logerror\",\n",
    "        data = train_imputed.sample(1000, random_state = 14), \n",
    "        hue = \"month_clusters\",\n",
    "        # hue_order = [0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        palette = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6']\n",
    ")\n",
    "\n",
    "plt.title(\"Predicted: Clusters of Transaction Month & Property Sq. Feet\")\n",
    "plt.legend(loc = 'upper left', ncol = 3)\n",
    "plt.ylim(-.2, .2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### **``Cluster Number 2: Home Size Cluster``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setting features to cluster on \n",
    "\n",
    "X_home_size = train_dummy[[ \n",
    "    'home_age',\n",
    "    'living_sqfeet_binned_360_to_1240_sqfeet',\n",
    "    'living_sqfeet_binned_1241_to_1565_sqfeet',\n",
    "    'living_sqfeet_binned_1566_to_2036_sqfeet',\n",
    "    'living_sqfeet_binned_2037_to_3855_sqfeet'\n",
    "]]\n",
    "\n",
    "val_home_size = validate_dummy[[ \n",
    "    'home_age',\n",
    "    'living_sqfeet_binned_360_to_1240_sqfeet',\n",
    "    'living_sqfeet_binned_1241_to_1565_sqfeet',\n",
    "    'living_sqfeet_binned_1566_to_2036_sqfeet',\n",
    "    'living_sqfeet_binned_2037_to_3855_sqfeet'\n",
    "]]\n",
    "\n",
    "test_home_size = test_dummy[[ \n",
    "    'home_age',\n",
    "    'living_sqfeet_binned_360_to_1240_sqfeet',\n",
    "    'living_sqfeet_binned_1241_to_1565_sqfeet',\n",
    "    'living_sqfeet_binned_1566_to_2036_sqfeet',\n",
    "    'living_sqfeet_binned_2037_to_3855_sqfeet'\n",
    "]]\n",
    "\n",
    "X_home_size.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the KMeans cluster object\n",
    "\n",
    "kmeans = KMeans(n_clusters = 4)\n",
    "kmeans.fit(X_home_size)\n",
    "\n",
    "clusters = kmeans.predict(X_home_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans Elbow-method\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    pd.Series({k: KMeans(k).fit(X_home_size).inertia_ for k in range(2, 12)}).plot(marker = 'o')\n",
    "    plt.xticks(range(2, 13))\n",
    "    \n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('inertia')\n",
    "    plt.title('Change in inertia as k increases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting inertia for number of KMeans clusters\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(13, 13), sharex=True, sharey=True)\n",
    "\n",
    "for ax, k in zip(axs.ravel(), range(2, 8)):\n",
    "    # creating and fitting KMeans\n",
    "    X2 = X_home_size.sample(2000, random_state = 14)\n",
    "    clusters = KMeans(k).fit(X2).predict(X2)\n",
    "\n",
    "    X2[\"logerror\"] = train_dummy[\"logerror\"]\n",
    "\n",
    "    ax.scatter(\n",
    "        X2[\"home_age\"], \n",
    "        X2[\"logerror\"], \n",
    "        c = clusters,\n",
    "        s = 2)\n",
    "\n",
    "    ax.set(title='k = {}'.format(k), xlabel = 'home age', ylabel = 'logerror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating clusters for ea. dataset\n",
    "\n",
    "train_size_clusters = kmeans.predict(X_home_size)\n",
    "val_size_clusters = kmeans.predict(val_home_size)\n",
    "test_size_clusters = kmeans.predict(test_home_size)\n",
    "\n",
    "pd.DataFrame(train_size_clusters).rename(columns = {0: \"Cluster Sample\"}).sample(10, random_state = 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding clusters to original datasets\n",
    "\n",
    "train_imputed[\"size_clusters\"] = train_size_clusters\n",
    "validate_imputed[\"size_clusters\"] = val_size_clusters\n",
    "test_imputed[\"size_clusters\"] = test_size_clusters\n",
    "\n",
    "train_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing how clusters compare against actual month & home age plots\n",
    "\n",
    "plt.subplots(1, 2, figsize = (16, 6), sharex = True, sharey = False)\n",
    "\n",
    "plt.subplot(121)\n",
    "sns.scatterplot(\n",
    "        x = \"home_age\",\n",
    "        y = \"logerror\",\n",
    "        data = train_imputed.sample(300, random_state = 14),\n",
    "        hue = \"living_sqfeet_binned\",\n",
    "        # hue_order = [\"360_to_1240_sqfeet\", \"1241_to_1565_sqfeet\", \"1566_to_2036_sqfeet\", \"2037_to_3855_sqfeet\"],\n",
    "        palette = ['#66c2a5','#fc8d62','#8da0cb','#e78ac3']\n",
    ")\n",
    "\n",
    "plt.title(\"Actual: Home Size Binned & Home Age\")\n",
    "plt.legend(loc = 'upper left', ncol = 2)\n",
    "plt.ylim(-.2, .2)\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.scatterplot(\n",
    "        x = \"home_age\",\n",
    "        y = \"logerror\",\n",
    "        data = train_imputed.sample(300, random_state = 14), \n",
    "        hue = \"size_clusters\",\n",
    "        # hue_order = [0, 1, 2, 3],\n",
    "        palette = ['#66c2a5','#fc8d62','#8da0cb','#e78ac3']\n",
    ")\n",
    "\n",
    "plt.title(\"Predicted: Clusters of Home Size Binned & Home Age\")\n",
    "plt.legend(loc = 'upper left', ncol = 2)\n",
    "plt.ylim(-.2, .2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### **``Cluster Number 3: Build Era Clusters``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setting features to cluster on \n",
    "\n",
    "X_era = train_dummy[[\n",
    "    'living_sq_feet',\n",
    "    'home_age_binned_new_century',\n",
    "    'home_age_binned_late_20th_century',\n",
    "    'home_age_binned_mid_20th_century',\n",
    "    'home_age_binned_early_20th_century']]\n",
    "\n",
    "val_era = validate_dummy[[\n",
    "    'living_sq_feet',\n",
    "    'home_age_binned_new_century',\n",
    "    'home_age_binned_late_20th_century',\n",
    "    'home_age_binned_mid_20th_century',\n",
    "    'home_age_binned_early_20th_century']]\n",
    "\n",
    "test_era = test_dummy[[\n",
    "    'living_sq_feet',\n",
    "    'home_age_binned_new_century',\n",
    "    'home_age_binned_late_20th_century',\n",
    "    'home_age_binned_mid_20th_century',\n",
    "    'home_age_binned_early_20th_century']]\n",
    "\n",
    "X_era.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the KMeans cluster object\n",
    "\n",
    "kmeans = KMeans(n_clusters = 4)\n",
    "kmeans.fit(X_era)\n",
    "\n",
    "clusters = kmeans.predict(X_era)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans Elbow-method\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    pd.Series({k: KMeans(k).fit(X_era).inertia_ for k in range(1, 12)}).plot(marker = 'o')\n",
    "    plt.xticks(range(2, 13))\n",
    "    \n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('inertia')\n",
    "    plt.title('Change in inertia as k increases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting inertia for number of KMeans clusters\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(13, 13), sharex=True, sharey=True)\n",
    "\n",
    "for ax, k in zip(axs.ravel(), range(2, 6)):\n",
    "\n",
    "    X3 = X_era.sample(2000, random_state = 54)\n",
    "\n",
    "    # creating and fitting KMeans\n",
    "    clusters = KMeans(k).fit(X3).predict(X3)\n",
    "\n",
    "    X3[\"logerror\"] = train_dummy[\"logerror\"]\n",
    "\n",
    "    ax.scatter(\n",
    "        X3[\"living_sq_feet\"], \n",
    "        X3[\"logerror\"], \n",
    "        c = clusters,\n",
    "        s = 2)\n",
    "\n",
    "    ax.set(title='k = {}'.format(k), xlabel = 'living sq. feet', ylabel = 'logerror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_era_clusters = kmeans.predict(X_era)\n",
    "validate_era_clusters = kmeans.predict(val_era)\n",
    "test_era_clusters = kmeans.predict(test_era)\n",
    "\n",
    "pd.DataFrame(train_era_clusters).rename(columns = {0: \"Cluster Sample\"}).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding clusters to original datasets\n",
    "\n",
    "train_imputed[\"era_clusters\"] = train_era_clusters\n",
    "validate_imputed[\"era_clusters\"] = validate_era_clusters\n",
    "test_imputed[\"era_clusters\"] = test_era_clusters\n",
    "\n",
    "train_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing how clusters compare against actual month & home age plots\n",
    "\n",
    "plt.subplots(1, 2, figsize = (16, 6), sharex = True, sharey = False)\n",
    "\n",
    "plt.subplot(121)\n",
    "sns.scatterplot(\n",
    "        x = \"living_sq_feet\",\n",
    "        y = \"logerror\",\n",
    "        data = train_imputed.sample(1000, random_state = 54),\n",
    "        hue = \"home_age_binned\",\n",
    "        # hue_order = [\"early_20th_century\", \"late_20th_century\", \"mid_20th_century\", \"new_century\"],\n",
    "        palette = ['#66c2a5','#fc8d62','#8da0cb','#e78ac3']\n",
    ")\n",
    "\n",
    "plt.title(\"Actual: Home Build Era by Living Sq. Feet\")\n",
    "plt.legend(loc = 'upper right', ncol = 2)\n",
    "plt.ylim(-.2, .2)\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.scatterplot(\n",
    "        x = \"living_sq_feet\",\n",
    "        y = \"logerror\",\n",
    "        data = train_imputed.sample(1000, random_state = 54), \n",
    "        hue = \"era_clusters\",\n",
    "        # hue_order = [0, 1, 2, 3],\n",
    "        palette = ['#66c2a5','#fc8d62','#8da0cb','#e78ac3']\n",
    ")\n",
    "\n",
    "plt.title(\"Predicted: Clusters of Home Build Era & Living Sq. Feet\")\n",
    "plt.legend(loc = 'upper right', ncol = 2)\n",
    "plt.ylim(-.2, .2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting size and era clusters against actual home value\n",
    "\n",
    "sns.relplot(\n",
    "    data = train_imputed.sample(500, random_state = 54), \n",
    "    x = \"home_value\", \n",
    "    y = \"logerror\", \n",
    "    col = \"era_clusters\", \n",
    "    hue = \"size_clusters\",\n",
    "    palette = ['#a6cee3','#1f78b4','#b2df8a','#33a02c'],\n",
    "    col_wrap = 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### **``Modeling``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummy dataframes with generated clusters for modeling\n",
    "\n",
    "train_dummy, validate_dummy, test_dummy = get_cluster_dummy(train_imputed, validate_imputed, test_imputed)\n",
    "print(f'dataframe shape: {train_dummy.shape}')\n",
    "train_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establishing a mean logerror baseline for train & validate datasets\n",
    "\n",
    "train_dummy, validate_dummy = acquire.establish_baseline(train_dummy, validate_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### ``2nd Data Split: Taking forward needed X variables and y (logerror) variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd split: splitting larger datasets into x and y variables\n",
    "\n",
    "X_train = train_dummy.drop(columns = [\n",
    "    \"logerror\", \n",
    "    \"home_value\",\n",
    "    'blockgroup_assignment',\n",
    "    'parcel_id',\n",
    "    'transaction_quarter',\n",
    "    'county_zoning_code',\n",
    "    'home_age_binned_new_century',               \n",
    "    'home_age_binned_late_20th_century',             \n",
    "    'home_age_binned_mid_20th_century',               \n",
    "    'home_age_binned_early_20th_century',                 \n",
    "    'living_sqfeet_binned_360_to_1240_sqfeet',\n",
    "    'living_sqfeet_binned_1241_to_1565_sqfeet',\n",
    "    'living_sqfeet_binned_1566_to_2036_sqfeet',\n",
    "    'living_sqfeet_binned_2037_to_3855_sqfeet',\n",
    "    'transaction_month_April',                      \n",
    "    'transaction_month_August',                      \n",
    "    'transaction_month_February',                    \n",
    "    'transaction_month_January',                     \n",
    "    'transaction_month_July',                       \n",
    "    'transaction_month_June',                        \n",
    "    'transaction_month_March',                       \n",
    "    'transaction_month_May',                         \n",
    "    'transaction_month_September',\n",
    "    'county_by_fips_LA County',\n",
    "    'county_by_fips_Orange County',\n",
    "    'county_by_fips_Ventura County',\n",
    "    'bathroom_count_1.0',\n",
    "    'bathroom_count_1.5',\n",
    "    'bathroom_count_2.0',\n",
    "    'bathroom_count_2.5',\n",
    "    'bathroom_count_3.0',\n",
    "    'bathroom_count_3.5',\n",
    "    'bathroom_count_4.0',\n",
    "    'bathroom_count_4.5',\n",
    "    'bedroom_count_1.0',\n",
    "    'bedroom_count_2.0',\n",
    "    'bedroom_count_3.0',\n",
    "    'bedroom_count_4.0',\n",
    "    'bedroom_count_5.0',\n",
    "    \"year_built\",\n",
    "    \"home_age\",\n",
    "    \"living_sq_feet\",\n",
    "    \"property_sq_feet\",\n",
    "    \"latitude\",\n",
    "    \"longitude\"\n",
    "])\n",
    "\n",
    "y_train = train_dummy[\"logerror\"]\n",
    "\n",
    "X_validate = validate_dummy.drop(columns = [\n",
    "    \"logerror\", \n",
    "    \"home_value\",\n",
    "    'blockgroup_assignment',\n",
    "    'parcel_id',\n",
    "    'transaction_quarter',\n",
    "    'county_zoning_code',\n",
    "    'home_age_binned_new_century',               \n",
    "    'home_age_binned_late_20th_century',             \n",
    "    'home_age_binned_mid_20th_century',               \n",
    "    'home_age_binned_early_20th_century',                 \n",
    "    'living_sqfeet_binned_360_to_1240_sqfeet',\n",
    "    'living_sqfeet_binned_1241_to_1565_sqfeet',\n",
    "    'living_sqfeet_binned_1566_to_2036_sqfeet',\n",
    "    'living_sqfeet_binned_2037_to_3855_sqfeet',\n",
    "    'transaction_month_April',                      \n",
    "    'transaction_month_August',                      \n",
    "    'transaction_month_February',                    \n",
    "    'transaction_month_January',                     \n",
    "    'transaction_month_July',                       \n",
    "    'transaction_month_June',                        \n",
    "    'transaction_month_March',                       \n",
    "    'transaction_month_May',                         \n",
    "    'transaction_month_September',\n",
    "    'county_by_fips_LA County',\n",
    "    'county_by_fips_Orange County',\n",
    "    'county_by_fips_Ventura County',\n",
    "    'bathroom_count_1.0',\n",
    "    'bathroom_count_1.5',\n",
    "    'bathroom_count_2.0',\n",
    "    'bathroom_count_2.5',\n",
    "    'bathroom_count_3.0',\n",
    "    'bathroom_count_3.5',\n",
    "    'bathroom_count_4.0',\n",
    "    'bathroom_count_4.5',\n",
    "    'bedroom_count_1.0',\n",
    "    'bedroom_count_2.0',\n",
    "    'bedroom_count_3.0',\n",
    "    'bedroom_count_4.0',\n",
    "    'bedroom_count_5.0',\n",
    "    \"year_built\",\n",
    "    \"home_age\",\n",
    "    \"living_sq_feet\",\n",
    "    \"property_sq_feet\",\n",
    "    \"latitude\",\n",
    "    \"longitude\"\n",
    "])\n",
    "\n",
    "y_validate = validate_dummy[\"logerror\"]\n",
    "\n",
    "X_test = test_dummy.drop(columns = [\n",
    "    \"logerror\", \n",
    "    \"home_value\",\n",
    "    'blockgroup_assignment',\n",
    "    'parcel_id',\n",
    "    'transaction_quarter',\n",
    "    'county_zoning_code',\n",
    "    'home_age_binned_new_century',               \n",
    "    'home_age_binned_late_20th_century',             \n",
    "    'home_age_binned_mid_20th_century',               \n",
    "    'home_age_binned_early_20th_century',                 \n",
    "    'living_sqfeet_binned_360_to_1240_sqfeet',\n",
    "    'living_sqfeet_binned_1241_to_1565_sqfeet',\n",
    "    'living_sqfeet_binned_1566_to_2036_sqfeet',\n",
    "    'living_sqfeet_binned_2037_to_3855_sqfeet',\n",
    "    'transaction_month_April',                      \n",
    "    'transaction_month_August',                      \n",
    "    'transaction_month_February',                    \n",
    "    'transaction_month_January',                     \n",
    "    'transaction_month_July',                       \n",
    "    'transaction_month_June',                        \n",
    "    'transaction_month_March',                       \n",
    "    'transaction_month_May',                         \n",
    "    'transaction_month_September',\n",
    "    'county_by_fips_LA County',\n",
    "    'county_by_fips_Orange County',\n",
    "    'county_by_fips_Ventura County',\n",
    "    'bathroom_count_1.0',\n",
    "    'bathroom_count_1.5',\n",
    "    'bathroom_count_2.0',\n",
    "    'bathroom_count_2.5',\n",
    "    'bathroom_count_3.0',\n",
    "    'bathroom_count_3.5',\n",
    "    'bathroom_count_4.0',\n",
    "    'bathroom_count_4.5',\n",
    "    'bedroom_count_1.0',\n",
    "    'bedroom_count_2.0',\n",
    "    'bedroom_count_3.0',\n",
    "    'bedroom_count_4.0',\n",
    "    'bedroom_count_5.0',\n",
    "    \"year_built\",\n",
    "    \"home_age\",\n",
    "    \"living_sq_feet\",\n",
    "    \"property_sq_feet\",\n",
    "    \"latitude\",\n",
    "    \"longitude\"\n",
    "])\n",
    "    \n",
    "y_test = test_dummy[\"logerror\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the shape\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquire.recursive_feature_eliminate(X_train, y_train, 10).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn's RFECV function to select best features to include\n",
    "# initiating, and fitting\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator = LinearRegression(),\n",
    "    min_features_to_select = 5)\n",
    "\n",
    "rfecv = rfecv.fit(X_train, y_train)\n",
    "\n",
    "feature_lst = X_train.columns[rfecv.support_].tolist()\n",
    "pd.DataFrame(feature_lst).rename(columns = {0: \"Features\"}).sort_values(\"Features\").reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only identified features from RFECV \n",
    "\n",
    "X_train = X_train[feature_lst]\n",
    "X_validate = X_validate[feature_lst]\n",
    "X_test = X_test[feature_lst]\n",
    "\n",
    "# checking the shape \n",
    "\n",
    "print(f'dataframe shape: {X_train.shape}')\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating and plotting feature importance\n",
    "plt.figure(figsize = (10, 5))\n",
    "sns.set(style = \"darkgrid\", font_scale = .75)\n",
    "\n",
    "rf = RandomForestRegressor(random_state = 123)\n",
    "rf = rf.fit(X_train, y_train)\n",
    "\n",
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "\n",
    "sns.barplot(rf.feature_importances_[sorted_idx], X_train.columns[sorted_idx], orient = \"h\", color = \"b\")\n",
    "\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### <u>**``Model Plots and Evaluation on Validate Dataset:``**</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.ceil(n * multiplier) / multiplier\n",
    "\n",
    "round_up(y_validate.mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating a dataframe w/X variables, y_train, and model predictions \n",
    "\n",
    "# creating the independent and dependent variables\n",
    "X_var = pd.DataFrame(X_validate[feature_lst])\n",
    "y_var = pd.DataFrame({'logerror actual': y_validate})\n",
    "predictions = pd.concat([X_var, y_var], axis = 1).reset_index(drop = True)\n",
    "\n",
    "# baseline mean predictions\n",
    "baseline = round_up(y_validate.mean(), 2)\n",
    "predictions[\"baseline_mean_predictions\"] = baseline\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### <u>**``Linear Models:``**</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating models using selected RFECV features\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr_model = lr.fit(X_train, y_train)\n",
    "\n",
    "lars = LassoLars()\n",
    "lars_model = lars.fit(X_train, y_train)\n",
    "\n",
    "glm = TweedieRegressor(alpha = 1, power = 0)\n",
    "glm_model = glm.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training R-squared w/Linear Regression:\", lr_model.score(X_train, y_train).round(4))\n",
    "print(\"Training R-squared w/Lasso Lars:\", lars_model.score(X_train, y_train).round(4))\n",
    "print(\"Training R-squared w/Tweedie Regressor:\", glm_model.score(X_train, y_train).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### <u>**``Non-linear Models:``**</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the Principal Competent Analysis \"PCA\" non-linear object\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X_reduced_train = pca.fit_transform(X_train)\n",
    "\n",
    "# transforming validate dataset\n",
    "X_reduced_validate = pca.transform(X_validate)\n",
    "\n",
    "# training PCR model on training data\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_reduced_train, y_train)\n",
    "\n",
    "# making predictions on validate dataset\n",
    "predictions[\"pca_predictions\"] = regr.predict(X_reduced_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial non-linear model\n",
    "# step 1: generating Polynomial Features\n",
    "\n",
    "poly = PolynomialFeatures(degree = 2, include_bias = False)\n",
    "poly.fit(X_train)\n",
    "\n",
    "X_train_poly = pd.DataFrame(\n",
    "    poly.transform(X_train),\n",
    "    columns = poly.get_feature_names(X_train.columns),\n",
    "    index = train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: using the poly features to create the linear regression model\n",
    "\n",
    "lm_poly = LinearRegression()\n",
    "lm_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "X_validate_poly = poly.transform(X_validate)\n",
    "predictions['polynomial degree 2'] = lm_poly.predict(X_validate_poly)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating pca regression model\n",
    "\n",
    "model_performance = []\n",
    "\n",
    "train_model = regr.predict(X_reduced_train)\n",
    "rmse_train = sqrt(mean_squared_error(y_train, train_model))\n",
    "\n",
    "validate_model = regr.predict(X_reduced_validate)\n",
    "rmse_validate = sqrt(mean_squared_error(y_validate, validate_model))\n",
    "\n",
    "metrics = {\n",
    "    \"model\": \"PCA\",\n",
    "    \"train_rmse\": rmse_train,\n",
    "    \"validate_rmse\": rmse_validate}\n",
    "\n",
    "model_performance.append(metrics)\n",
    "\n",
    "print('RMSE for PCA model on the train dataset: {:.2f}'.format(rmse_train))\n",
    "print('RMSE for PCA model on the validate dataset: {:.2f}'.format(rmse_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating polynomial deg. 2 regression model\n",
    "\n",
    "train_model = lm_poly.predict(X_train_poly)\n",
    "rmse_train = sqrt(mean_squared_error(y_train, train_model))\n",
    "\n",
    "validate_model = lm_poly.predict(X_validate_poly)\n",
    "rmse_validate = sqrt(mean_squared_error(y_validate, validate_model))\n",
    "\n",
    "metrics = {\n",
    "    \"model\": \"Polynomial deg.2\",\n",
    "    \"train_rmse\": rmse_train,\n",
    "    \"validate_rmse\": rmse_validate}\n",
    "\n",
    "model_performance.append(metrics)\n",
    "\n",
    "print('RMSE for Polynomial Deg. 2 model on the train dataset: {:.2f}'.format(rmse_train))\n",
    "print('RMSE for Polynomial Deg. 2 model on the validate dataset: {:.2f}'.format(rmse_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating linear regression models\n",
    "\n",
    "models = [lr_model, lars_model, glm_model]\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    train_model = model.predict(X_train)\n",
    "    rmse_train = sqrt(mean_squared_error(y_train,\n",
    "                                         train_model))\n",
    "    \n",
    "    validate_model = model.predict(X_validate)\n",
    "    rmse_validate = sqrt(mean_squared_error(y_validate,\n",
    "                                         validate_model))\n",
    "    metrics = {\n",
    "    \"model\": str(model),\n",
    "    \"train_rmse\": rmse_train,\n",
    "    \"validate_rmse\": rmse_validate}\n",
    "\n",
    "    model_performance.append(metrics)\n",
    "\n",
    "    print('RMSE for {} model on the train dataset: {}'.format(model, round_up(rmse_train, 2)))\n",
    "    print('RMSE for {} model on the validate dataset: {}'.format(model, round_up(rmse_validate, 2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning the models performance \n",
    "\n",
    "model_performance = pd.DataFrame(model_performance)\n",
    "model_performance[\"model\"] = model_performance[\"model\"].replace(\n",
    "    {\"LinearRegression()\": \"LinearRegression\", \n",
    "     \"LassoLars()\": \"LassoLars\", \n",
    "     \"TweedieRegressor(alpha=1, power=0)\": \"TweedieRegressor\"})\n",
    "\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generating validate model predictions and assigning to dataframe\n",
    "\n",
    "lr_predictions = lr_model.predict(X_validate)\n",
    "predictions[\"linear_predictions\"] = lr_predictions.round(4)\n",
    "\n",
    "lars_predictions = lars_model.predict(X_validate)\n",
    "predictions[\"lars_predictions\"] = lars_predictions.round(4)\n",
    "\n",
    "glm_predictions = lars_model.predict(X_validate)\n",
    "predictions[\"glm_predictions\"] = glm_predictions.round(4)\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melting columns for residual plot\n",
    "melt_df = acquire.get_melted_table(predictions)\n",
    "\n",
    "# plotting model residuals\n",
    "acquire.plot_model_residuals(melt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting model predicted home values against target\n",
    "acquire.plot_models(melt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model logerror distribution\n",
    "acquire.model_distributions(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a dataframe\n",
    "test_df = pd.DataFrame(y_test)\n",
    "\n",
    "# using poly linear model to transform X_test\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# generating model predictions\n",
    "test_df[\"model_predictions\"] = lm_poly.predict(X_test_poly)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning R-squared score & RMSE on test dataset\n",
    "\n",
    "rmse_test = sqrt(mean_squared_error(test_df['logerror'], test_df['model_predictions']))\n",
    "\n",
    "# print('Training R-squared w/Linear Model: {:.3f}'.format(lr_model.score(X_test, y_test)))\n",
    "print('RMSE for Polynomial Deg. 2 model on the test dataset: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning RMSE report \n",
    "acquire.final_rmse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
